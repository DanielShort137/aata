%%%%(c)
%%%%(c)  This file is a portion of the source for the textbook
%%%%(c)
%%%%(c)    Abstract Algebra: Theory and Applications
%%%%(c)    Copyright 1997 by Thomas W. Judson
%%%%(c)
%%%%(c)  See the file COPYING.txt for copying conditions
%%%%(c)
%%%%(c)
\chapter{Vector Spaces}\label{vect}


 
In a physical system a quantity can often be described with a single
number. For example, we need to know only a single number to describe
temperature, mass, or volume.  However, for some quantities, such as
location, we need several numbers. To give the location of a point in
space, we need $x$, $y$, and $z$ coordinates. Temperature
distribution over a solid object requires four numbers: three to
identify each point within the object and a fourth to describe the
temperature at that point.  Often $n$-tuples of numbers, or vectors,
also have certain algebraic properties, such as addition or scalar 
multiplication.  


In this chapter we will examine mathematical structures called vector
spaces. As with groups and rings, it is desirable to give a simple
list of axioms that must be satisfied to make a set of vectors a
structure worth studying.  
 
 
 
\section{Definitions and Examples}
 

A {\bfi vector space\/}\index{Vector space!definition of} $V$ over a
field $F$ is an abelian group with a {\bfi scalar
product\/}\index{Scalar product} $\alpha \cdot v$ or $\alpha v$ defined
for all $\alpha \in F$ and all $v \in V$ satisfying the following
axioms.  
\begin{itemize}

\item 
$\alpha(\beta v) =(\alpha \beta)v$;

\item 
$(\alpha + \beta)v =\alpha v + \beta v$;

\item 
$\alpha(u + v) = \alpha u + \alpha v$;

\item 
$1v=v$;

\end{itemize}
where $\alpha, \beta \in F$ and $u, v \in V$.
 

The elements of $V$ are called {\bfi vectors}; the elements of $F$
are called {\bfi scalars}.  It is important to notice that in most
cases two vectors cannot be multiplied.  In general, it is only
possible to multiply a vector with a scalar. To differentiate between
the scalar zero and the vector zero, we will write them as 0 and
${\bold 0}$, respectively.  


Let us examine several examples of vector spaces. Some of them will be
quite familiar; others will seem less so.
 
 
\vspace{2 ex}

 
\noindent {\bf Example 1.}
The $n$-tuples of real numbers, denoted by ${\Bbb R}^n$, form a vector
space over ${\Bbb R}$. Given vectors $u = (u_1, \ldots, u_n)$ and $v =
(v_1, \ldots, v_n)$ in ${\Bbb R}^n$ and $\alpha$ in ${\Bbb R}$, we can
define vector addition by
\[
u + v = (u_1, \ldots, u_n) + (v_1, \ldots, v_n)
=
(u_1 + v_1, \ldots, u_n + v_n)
\]
and scalar multiplication by 
\[
\alpha u = \alpha(u_1, \ldots, u_n)= (\alpha u_1, \ldots, \alpha u_n).
\]
\hspace{\fill} $\blacksquare$
 
 
\vspace{2 ex}
 
  
\noindent {\bf Example 2.}
If $F$ is a field, then $F[x]$ is a vector space over $F$. The vectors
in $F[x]$ are simply polynomials.  Vector addition is just polynomial
addition. If $\alpha \in F$ and $p(x) \in F[x]$, then scalar
multiplication is defined by $\alpha p(x)$.
\hspace{\fill} $\blacksquare$
 
 
\vspace{2 ex}
 
 
\noindent {\bf Example 3.}
The set of all continuous real-valued functions on a closed interval
$[a,b]$ is a vector space over ${\Bbb R}$.  If $f(x)$ and $g(x)$ are
continuous on $[a, b]$, then $(f+g)(x)$ is defined to be $f(x) +
g(x)$.  Scalar multiplication is defined by
$(\alpha f)(x) = \alpha f(x)$ for $\alpha \in 
{\Bbb R}$. For example, if $f(x) = \sin x$ and $g(x)= x^2$, then 
$(2f+5g)(x) =2 \sin x + 5 x^2$. 
\hspace{\fill} $\blacksquare$
 

\vspace{2 ex}
 
 
\noindent {\bf Example 4.}
Let $V = {\Bbb Q}(\sqrt{2}\, ) = \{ a + b \sqrt{2} : a, b \in 
{\Bbb Q } \}$. Then $V$ is a
vector space over ${\Bbb Q}$. If $u = a + b \sqrt{2}$ and $v = c + d
\sqrt{2}$, then $u + v = (a + c) + (b + d ) \sqrt{2}$ is again in $V$.
Also, for $\alpha \in {\Bbb Q}$, $\alpha v$ is in $V$.  We will leave
it as an exercise to verify that all of the vector space axioms hold
for $V$. 
\hspace{\fill} $\blacksquare$

 
\begin{proposition}
Let $V$ be a vector space over $F$. Then each of the following
statements is true. 
\begin{enumerate}

\rm \item \it 
$0v ={\bold 0}$ for all $v \in V$.

\rm \item \it 
$\alpha {\bold 0} = {\bold 0}$ for all $\alpha \in F$.


\rm \item \it 
If $\alpha v = {\bold 0}$, then either $\alpha = 0$ or $v = {\bold
0}$.  

\rm \item \it
$(-1) v = -v$ for all $v \in V$.

\rm \item \it 
$-(\alpha v) = (-\alpha)v = \alpha(-v)$ for all $\alpha \in F$ and all
$v \in V$. 

\end{enumerate}
\end{proposition}


\begin{proof}
To prove (1), observe that 
\[
0 v = (0 + 0)v = 0v + 0v;
\]
consequently, ${\bold 0} + 0 v = 0v + 0v$. Since $V$ is an abelian
group, ${\bold 0} = 0v$. 


The proof of (2) is almost identical to the proof of (1). For (3), we
are done if $\alpha = 0$.  Suppose that $\alpha \neq 0$. Multiplying
both sides of $\alpha v = {\bold 0}$ by $1/ \alpha$, we have $v =
{\bold 0}$.


To show (4), observe that
\[
v + (-1)v = 1v + (-1)v = (1-1)v = 0v = {\bold 0},
\]
and so $-v = (-1)v$. We will leave the proof of (5) as an exercise.
\end{proof}
 
 
 
\section{Subspaces}


Just as groups have subgroups and rings have subrings, vector spaces
also have substructures. Let $V$ be a vector space over a field $F$,
and $W$ a subset of $V$. Then $W$ is a {\bfi subspace\/}\index{Vector
space!subspace of} of $V$ if it is closed under vector addition and
scalar multiplication; that is, if $u, v \in W$ and $\alpha
\in F$, it will always be the case that $u+v$ and $\alpha v$ are also
in $W$.   
 

\vspace{2 ex}
 
  
\noindent {\bf Example 5.}
Let $W$ be the subspace of ${\Bbb R}^3$ defined by $W = \{ (x_1, 2 x_1
+ x_2, x_1 - x_2) : x_1, x_2 \in {\Bbb R} \}$. We claim that $W$ is a 
subspace of ${\Bbb R}^3$.  Since 
\begin{eqnarray*}
\alpha (x_1, 2 x_1 + x_2, x_1 - x_2) 
& = & (\alpha x_1, \alpha(2 x_1 + x_2), \alpha( x_1 - x_2)) \\
& = & (\alpha x_1, 2(\alpha x_1) + \alpha x_2, \alpha x_1 -\alpha x_2),
\end{eqnarray*}
$W$ is closed under scalar multiplication. To show that $W$ is closed
under vector addition, let $u = (x_1, 2 x_1 + x_2, x_1 - x_2)$ and $v
= (y_1, 2 y_1 + y_2, y_1 - y_2)$ be vectors in $W$. Then
\[
u + v = 
(x_1 + y_1, 2( x_1 + y_1) +( x_2 + y_2), (x_1 + y_1) - (x_2+ y_2)).
\]
\hspace{\fill} $\blacksquare$
 
 
\vspace{2 ex}
 
 
\noindent {\bf Example 6.}
Let $W$ be the subset of polynomials of $F[x]$ with no odd-power
terms. If $p(x)$ and $q(x)$ have no odd-power terms, then neither will 
$p(x) + q(x)$.  Also, $\alpha p(x) \in W$ for $\alpha \in F$ and $p(x)
\in W$.
\hspace{\fill} $\blacksquare$
  

\vspace{2 ex}

 
Let $V$ be any vector field over a field $F$ and suppose that $v_1,
v_2, \ldots, v_n$ are vectors in $V$ and $\alpha_1, \alpha_2, \ldots,
\alpha_n$ are scalars in $F$. Any vector $w$ in $V$ of the form
\[
w = \sum_{i=1}^n \alpha_i v_i = \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n
\]
is called a {\bfi linear combination\/}\index{Linear combination} of the
vectors $v_1, v_2, \ldots, v_n$. The {\bfi spanning
set\/}\index{Spanning set} of vectors $v_1, v_2, \ldots, v_n$ is the
set of vectors obtained from all possible linear combinations of
$v_1, v_2, \ldots, v_n$. If $W$ is the spanning set of $v_1, v_2,
\ldots, v_n$, then we often say that $W$ is {\bfi spanned\/} by $v_1,
v_2, \ldots, v_n$. 
 
 
\begin{proposition}
Let $S= \{v_1, v_2, \ldots, v_n \}$ be vectors in a vector space $V$.
Then the span of $S$ is a subspace of $V$. 
\end{proposition}


\begin{proof}
Let $u$ and $v$ be in $S$. We can write both of these vectors as 
linear combinations of the $v_i$'s:
\begin{eqnarray*}
u & = & \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n \\
v & = & \beta_1 v_1 + \beta_2 v_2 + \cdots + \beta_n v_n.
\end{eqnarray*}
Then
\[
u+ v =( \alpha_1 + \beta_1) v_1 + (\alpha_2+ \beta_2) v_2 + \cdots +
(\alpha_n + \beta_n) v_n 
\]
is a linear combination of the $v_i$'s. For $\alpha \in F$,
\[
\alpha u = (\alpha \alpha_1) v_1 + ( \alpha \alpha_2) v_2 + \cdots +
(\alpha \alpha_n ) v_n 
\]
is in the span of $S$.
\end{proof}


 
\section{Linear Independence}
 

Let $S = \{v_1, v_2, \ldots, v_n\}$ be a set of vectors in a vector
space $V$. If there exist scalars $\alpha_1, \alpha_2 \ldots \alpha_n
\in F$ such that not all of the $\alpha_i$'s are zero and 
\[
\alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n = {\bold 0 },
\]
then $S$ is said to be {\bfi linearly
dependent}\index{Linear dependence}. If the set $S$ is not linearly
dependent, then it is said to be {\bfi linearly
independent}\index{Linear independence}. More specifically, $S$ is a
linearly independent set if
\[ 
\alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n = {\bold 0 }
\]
implies that
\[
\alpha_1 = \alpha_2 = \cdots = \alpha_n = 0
\]
for any set of scalars $\{ \alpha_1, \alpha_2 \ldots \alpha_n \}$.


 
\begin{proposition}
Let $\{ v_1, v_2, \ldots, v_n \}$ be a set of linearly independent
vectors in a vector space. Suppose that 
\[
v = \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n
= \beta_1 v_1 + \beta_2 v_2 + \cdots + \beta_n v_n.
\]
Then $\alpha_1 = \beta_1, \alpha_2 = \beta_2, \ldots, \alpha_n =
\beta_n$. 
\end{proposition}

\begin{proof}
If 
\[
v = \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n
= \beta_1 v_1 + \beta_2 v_2 + \cdots + \beta_n v_n,
\]
then
\[
(\alpha_1 - \beta_1) v_1 + (\alpha_2 - \beta_2) v_2 + \cdots +
(\alpha_n - \beta_n) v_n = {\bold 0}.
\]
Since $v_1, \ldots, v_n$ are linearly independent, $\alpha_i - \beta_i
=0$ for $i = 1, \ldots, n$.
\end{proof}
 

\vspace{2ex}


The definition of linear dependence makes more sense if we consider
the following proposition.

 
\begin{proposition}
A set $\{ v_1, v_2, \dots, v_n \}$ of vectors in a vector space $V$ is
linearly dependent if and only if one of the $v_i$'s is a linear
combination of the rest. 
\end{proposition}


\begin{proof}
Suppose that $\{ v_1, v_2, \dots, v_n \}$ is a set of linearly dependent
vectors.  Then there exist scalars $\alpha_1, \ldots, \alpha_n$
such that
\[
\alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n = {\bold 0 },
\]
with at least one of the $\alpha_i$'s not equal to zero.  Suppose that
$\alpha_k \neq 0$. Then 
\[
v_k = - \frac{\alpha_1}{\alpha_k} v_1 
- \cdots 
- \frac{\alpha_{k-1}}{\alpha_k}	v_{k-1}
- \frac{\alpha_{k+1}}{\alpha_k}	v_{k+1}
- \cdots 
- \frac{\alpha_n}{\alpha_k} v_n.
\]


Conversely, suppose that 
\[
v_k = \beta_1 v_1 
+ \cdots 
+ \beta_{k-1} v_{k-1}
+ \beta_{k+1} v_{k+1}
+ \cdots 
+ \beta_n v_n.
\]
Then
\[
\beta_1 v_1 
+ \cdots 
+ \beta_{k-1} v_{k-1}
- v_k
+ \beta_{k+1} v_{k+1}
+ \cdots 
+ \beta_n v_n = {\bold 0}.
\]
\end{proof}

\vspace{2ex}


The following proposition is a consequence of the fact that any system
of homogeneous linear equations with more unknowns than equations will
have a nontrivial solution.  We leave the details of the proof for the
end-of-chapter exercises. 
 

\begin{proposition}
Suppose that a vector space $V$ is spanned by $n$ vectors. If $m > n$,
then any set of $m$ vectors in $V$ must be linearly dependent. 
\end{proposition}
 
  
A set $\{ e_1, e_2, \ldots, e_n \}$ of vectors in a vector space $V$
is called a {\bfi basis\/}\index{Vector space!basis of} for $V$ if $\{
e_1, e_2, \ldots, e_n \}$ is a linearly independent set that spans
$V$.

 
\vspace{2 ex}
 
 
\noindent {\bf Example 7.}
The vectors $e_1 = (1, 0, 0)$, $e_2 = (0, 1, 0)$, and $e_3 =(0, 0, 1)$
form a basis for ${\Bbb R}^3$.  The set certainly spans ${\Bbb R}^3$,
since any arbitrary vector $(x_1, x_2, x_3)$ in ${\Bbb R}^3$ can be
written as $x_1 e_1 + x_2 e_2 + x_3 e_3$. Also, none of the vectors
$e_1, e_2, e_3$ can be written as a linear combination of the other
two; hence, they are linearly independent.  The vectors $e_1, e_2,
e_3$ are not the only basis of ${\Bbb R}^3$:  the set $\{ (3, 2, 1),
(3, 2, 0), (1, 1, 1) \}$ is also a basis for ${\Bbb R}^3$. 
\hspace{\fill} $\blacksquare$

 
\vspace{2 ex}
 
 
\noindent {\bf Example 8.}
Let $\mbox{${\Bbb Q}( \sqrt{2}\, )$} = \{ a + b \sqrt{2} : a, b \in {\Bbb Q} \}$.
The sets $\{1, \sqrt{2}\,  \}$ and $\{1+\sqrt{2}, 1- \sqrt{2}\,  \}$ are
both bases of ${\Bbb Q}( \sqrt{2}\, )$.  
\hspace{\fill} $\blacksquare$


\vspace{2ex}


From the last two examples it should be clear that a given vector
space has several bases. In fact, there are an infinite number of
bases for both of these examples. {\em In general, there is no unique 
basis for a vector space}.  However, every basis of ${\Bbb R}^3$ consists
of exactly three vectors, and every  basis of ${\Bbb Q}(\sqrt{2}\, )$ 
consists of exactly two vectors. This is a consequence of the next 
proposition.


\begin{proposition}
Let $\{ e_1, e_2, \ldots, e_m \}$ and $\{ f_1, f_2, \ldots, f_n \}$ be
two bases for a vector space $V$. Then $m=n$. 
\end{proposition}


\begin{proof}
Since $\{ e_1, e_2, \ldots, e_m \}$ is a basis, it is a linearly
independent set.  By  Proposition~18.5, $n \leq m$. Similarly, $\{
f_1, f_2, \ldots, f_n \}$ is a linearly independent set, and the last
proposition implies that $m \leq n$.  Consequently, $m =n$.
\mbox{\hspace{1in}}
\end{proof}
 

\vspace{2ex}
 

If $\{ e_1, e_2, \ldots, e_n \}$ is a basis for a vector space $V$,
then we say that the {\bfi dimension\/}\index{Vector space!dimension of}
of $V$ is $n$ and we write $\dim V =n$\label{vectdim}. 
We will leave the proof of the following theorem as an exercise.


\begin{theorem}
Let $V$ be a vector space of dimension $n$.
\begin{enumerate}

\rm \item \it
If $S = \{v_1, \ldots, v_n \}$ is a set of linearly independent
vectors for $V$, then $S$ is a basis for $V$. 

\rm \item \it
If $S = \{v_1, \ldots, v_n \}$ spans $V$, then $S$ is a basis for $V$. 

\rm \item \it
If $S = \{v_1, \ldots, v_k \}$ is a set of linearly independent
vectors for $V$ with $k < n$, then there exist vectors $v_{k+1},
\ldots, v_n$ such that  
\[
\{v_1, \ldots, v_k, v_{k+1}, \ldots, v_n \}
\] 
is a basis for $V$. 

\end{enumerate}
\end{theorem}


 
\markright{EXERCISES}
\section*{Exercises}
\exrule

 
{\small
\begin{enumerate}

  
\bf\item\rm
If $F$ is a field, show that $F[x]$ is a vector space over $F$, where
the vectors in $F[x]$ are polynomials.  Vector addition is polynomial
addition, and scalar multiplication is defined by $\alpha p(x)$ for
$\alpha \in F$.  

\bf\item\rm
Prove that ${\Bbb Q }( \sqrt{2}\, )$ is a vector space.


\bf\item\rm
Let ${\Bbb Q }( \sqrt{2}, \sqrt{3}\, )$ be the field generated by
elements of the form $a + b \sqrt{2}  + c \sqrt{3}$, where $a, b, c$
are in ${\Bbb Q}$. Prove that ${\Bbb Q }( \sqrt{2}, \sqrt{3}\, )$ is a
vector space of dimension 4 over ${\Bbb Q}$.  Find a basis for 
${\Bbb Q }( \sqrt{2}, \sqrt{3}\, )$.


\bf\item\rm 
Prove that the complex numbers are a vector space of dimension 2
over ${\Bbb R}$. 


\bf\item\rm
Prove that the set $P_n$ of all polynomials of degree less than $n$
form a subspace of the vector space $F[x]$. Find a basis for $P_n$ and
compute the dimension of~$P_n$. 


\bf\item\rm
Let $F$ be a field and denote the set of $n$-tuples of $F$ by $F^n$.
Given vectors $u = (u_1, \ldots, u_n)$ and $v = (v_1, \ldots, v_n)$ in
$F^n$ and $\alpha$ in $F$, define vector addition by
\[
u + v = (u_1, \ldots, u_n) + (v_1, \ldots, v_n)
=
(u_1 + v_1, \ldots, u_n + v_n)
\]
and scalar multiplication by 
\[
\alpha u = \alpha(u_1, \ldots, u_n)= (\alpha u_1, \ldots, \alpha u_n).
\]
Prove that $F^n$ is a vector space of dimension $n$ under these
operations. 


\bf\item\rm
Which of the following sets are subspaces of ${\Bbb R}^3$? If the set
is indeed a subspace, find a basis for the subspace and compute its
dimension.
\begin{enumerate}

  \bf\item\rm
$\{ (x_1, x_2, x_3) : 3 x_1 - 2 x_2 + x_3 = 0 \}$

  \bf\item\rm
$\{ (x_1, x_2, x_3) : 3 x_1 + 4 x_3 = 0, 2 x_1 - x_2 + x_3 = 0 \}$

  \bf\item\rm
$\{ (x_1, x_2, x_3) : x_1 - 2 x_2 + 2 x_3 = 2 \}$

  \bf\item\rm
$\{ (x_1, x_2, x_3) : 3 x_1 - 2 x_2^2 = 0 \}$

\end{enumerate}


\bf\item\rm
Show that the set of all possible solutions $(x, y, z) \in {\Bbb R}^3$
of the equations
\begin{eqnarray*}
Ax + B y + C z & = & 0 \\
D x + E y + C z & = & 0
\end{eqnarray*}
forms a subspace of ${\Bbb R}^3$.


\bf\item\rm
Let $W$ be the subset of continuous functions on $[0, 1]$ such that
$f(0) = 0$.  Prove that $W$ is a subspace of $C[0, 1]$.


%*******************THEORY***********************


\bf\item\rm
Let $V$ be a vector space over $F$. Prove that $-(\alpha v) =
(-\alpha)v = \alpha(-v)$ for all $\alpha \in F$ and all $v \in V$. 


\bf\item\rm
Let $V$ be a vector space of dimension $n$. Prove each of the
following statements. 
\begin{enumerate}

 \bf\item\rm
If $S = \{v_1, \ldots, v_n \}$ is a set of linearly independent
vectors for $V$, then $S$ is a basis for $V$. 

 \bf\item\rm
If $S = \{v_1, \ldots, v_n \}$ spans $V$, then $S$ is a basis for $V$.

 \bf\item\rm 
If $S = \{v_1, \ldots, v_k \}$ is a set of linearly independent
vectors for $V$ with $k < n$, then there exist vectors $v_{k+1},
\ldots, v_n$ such that 
\[
\{v_1, \ldots, v_k, v_{k+1}, \ldots, v_n \}
\] 
is a basis for $V$. 

\end{enumerate}


\bf\item\rm
Prove that any set of vectors containing ${\bold 0}$ is linearly
dependent. 


\bf\item\rm
Let $V$ be a vector space. Show that $\{ {\bold 0} \}$ is a subspace
of $V$ of dimension zero.


\bf\item\rm
If a vector space $V$ is spanned by $n$ vectors, show that any set of
$m$ vectors in $V$ must be linearly dependent for $m >n$.  


\bf\item\rm
{\bf Linear Transformations.}
Let $V$ and $W$ be vector spaces over a field $F$, of dimensions $m$
and $n$, respectively. If $T: V \rightarrow W$ is a map satisfying
\begin{eqnarray*}
T( u+ v ) & = & T(u ) + T(v) \\
T( \alpha v ) & = & \alpha T(v)
\end{eqnarray*}
for all $\alpha \in F$ and all $u, v \in V$, then $T$ is called a
{\bfi linear transformation\/}\index{Linear transformation!definition
of} from $V$ into $W$. 
\begin{enumerate}

   \bf\item\rm
Prove that the {\bfi kernel\/}\index{Kernel!of a linear
transformation}\index{Linear transformation!kernel of} of $T$, 
$\ker(T) = \{ v \in V : T(v) = 
{\bold 0} \}$, is a subspace of $V$. The kernel of $T$ is sometimes
called the {\bfi null space\/}\index{Null space!of a linear
transformation}\index{Linear transformation!null space of} of $T$. 

   \bf\item\rm
Prove that the {\bfi range\/}\index{Linear transformation!range of} or
{\bfi range space\/} of $T$, $R(V) = \{ w \in W : T(v) = w \mbox{ for
some $v \in V$} \}$, is a subspace of $W$. 

   \bf\item\rm
Show that $T : V \rightarrow W$ is injective if and only if 
$\ker(T) = \{ \bold 0 \}$.

   \bf\item\rm
Let $\{ v_1, \ldots, v_k \}$ be a basis for the null space of $T$. We
can extend this basis to be a basis $\{ v_1, \ldots, v_k, v_{k+1},
\ldots, v_m\}$ of $V$. Why?  Prove that $\{ T(v_{k+1}), \ldots, T(v_m)
\}$ is a basis for the range of $T$. Conclude that the range of $T$
has dimension $m-k$.

   \bf\item\rm
Let $\dim V = \dim W$.  Show that a linear transformation $T : V
\rightarrow W$ is injective if and only if it is surjective.

\end{enumerate}


\bf\item\rm
Let $V$ and $W$ be finite dimensional vector spaces of dimension $n$
over a field $F$. Suppose that  $T: V \rightarrow W$ is a vector space
isomorphism.  If $\{ v_1, \ldots, v_n \}$ is a basis of $V$, show that
$\{ T(v_1), \ldots, T(v_n) \}$ is a basis of $W$. Conclude that any
vector space over a field $F$ of dimension $n$ is isomorphic to $F^n$. 


\bf\item\rm
{\bf Direct Sums.} 
Let $U$ and $V$ be subspaces of a vector space $W$. The sum of $U$ and
$V$, denoted $U + V$, is defined to be the set of all vectors of the
form $u + v$, where $u \in U$ and $v \in V$. 
\begin{enumerate}

   \bf\item\rm
Prove that $U + V$ and $U \cap V$ are subspaces of $W$.

   \bf\item\rm
If $U + V = W$ and $U \cap V = {\bold 0}$, then $W$ is said to be the
{\bfi direct sum\/}\index{Direct sum of vector spaces}\index{Vector
space!direct sum of} of $U$ and $V$ and we write $W = U \oplus
V$\label{notedirectsum}.
Show that every element $w \in W$ can be written uniquely as $w = u +
v$, where $u \in U$ and $v \in V$.

   \bf\item\rm
Let $U$ be a subspace of dimension $k$ of a vector space $W$ of
dimension $n$. Prove that there exists a subspace $V$ of dimension
$n-k$ such that $W = U \oplus V$.  Is the subspace $V$ unique?

   \bf\item\rm
If $U$ and $V$ are arbitrary subspaces of a vector space $W$, show
that 
\[
\dim( U + V) = \dim U + \dim V - \dim( U \cap V).
\]

\end{enumerate}


\bf\item\rm
{\bf Dual Spaces.} 
Let $V$ and $W$ be finite dimensional vector spaces over a field~$F$. 
\begin{enumerate}

   \bf\item\rm
Show that the set of all linear transformations from $V$ into $W$,
denoted by $\mbox{Hom}(V, W)$\label{noteHom}, 
is a vector space over $F$, where we
define vector addition as follows:
\begin{eqnarray*}
(S + T)(v) &= & S(v) +T(v) \\
(\alpha S)(v) & = & \alpha S(v),
\end{eqnarray*}
where $S, T \in \mbox{Hom}(V, W)$, $\alpha \in F$, and $v \in V$.
 
   \bf\item\rm
Let $V$ be an $F$-vector space.  Define the {\bfi dual
space\/}\index{Vector space!dual of} of $V$ 
to be $V^\ast = \mbox{Hom}(V, F)$\label{notedual}. Elements in the dual space of $V$ are
called {\bfi linear functionals}\index{Linear functionals}.  Let $v_1,
\ldots, v_n$ be an ordered basis for $V$. If $v = \alpha_1 v_1 +
\cdots + \alpha_n v_n$ is any vector in $V$, define a linear
functional  $\phi_i : V \rightarrow F$ by $\phi_i (v) = \alpha_i$.
Show that the $\phi_i$'s form a basis for $V^\ast$.  This basis is
called the {\bfi dual basis\/} of $v_1, \ldots, v_n$ (or simply the dual
basis if the context makes the meaning clear).  
 

   \bf\item\rm
Consider the basis $\{ (3, 1), (2, -2) \}$ for ${\Bbb R}^2$. What is
the dual basis for $({\Bbb R}^2)^\ast$? 
 
   \bf\item\rm
Let $V$ be a vector space of dimension $n$ over a field $F$ and let
$V^{\ast \ast}$ be the dual space $V^\ast$.  Show that each element $v
\in V$ gives rise to an element $\lambda_v$ in $V^{\ast \ast}$ and
that the map $v \mapsto \lambda_v$ is an isomorphism of $V$ with
$V^{\ast \ast}$. 

\end{enumerate}
 

\end{enumerate}
}


 
\subsection*{References and Suggested Readings}
 

{\small
\begin{itemize}
 
\item[{\bf [1]}]  %%%%%%%%%%%%%%checked
Curtis, C. W. {\it Linear Algebra: An Introductory Approach}.
Springer-Verlag, New York, 1984.
 
\item[{\bf [2]}]
Hoffman, K. and Kunze, R. {\it Linear Algebra}. 2nd ed.
Prentice-Hall, Englewood Cliffs, NJ, 1971.

\item[{\bf [3]}]
Johnson, L. W., Riess, R. D., and Arnold, J. T. {\it Introduction to
Linear Algebra}. 3rd ed. Addison-Wesley, Reading, MA, 1993.
 
\item[{\bf [4]}]
Leon, S. J. {\it Linear Algebra with Applications}. 3rd ed.
Macmillan, New York, 1990.
 
\item[{\bf [5]}]
Nicholson, W. K. {\it Elementary Linear Algebra with Applications}.
2nd ed. PWS-KENT, Boston, 1990.
 

\end{itemize}
}


